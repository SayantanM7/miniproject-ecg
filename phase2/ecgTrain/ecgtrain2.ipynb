{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afdadf22-48d6-4f1e-bd25-b2a388153dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-tuner\n",
      "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: keras in /home/sayantan/anaconda3/envs/ml/lib/python3.10/site-packages (from keras-tuner) (3.9.2)\n",
      "Requirement already satisfied: packaging in /home/sayantan/anaconda3/envs/ml/lib/python3.10/site-packages (from keras-tuner) (24.2)\n",
      "Requirement already satisfied: requests in /home/sayantan/anaconda3/envs/ml/lib/python3.10/site-packages (from keras-tuner) (2.32.3)\n",
      "Collecting kt-legacy (from keras-tuner)\n",
      "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
      "Requirement already satisfied: absl-py in /home/sayantan/anaconda3/envs/ml/lib/python3.10/site-packages (from keras->keras-tuner) (2.2.0)\n",
      "Requirement already satisfied: numpy in /home/sayantan/anaconda3/envs/ml/lib/python3.10/site-packages (from keras->keras-tuner) (1.26.4)\n",
      "Requirement already satisfied: rich in /home/sayantan/anaconda3/envs/ml/lib/python3.10/site-packages (from keras->keras-tuner) (13.9.4)\n",
      "Requirement already satisfied: namex in /home/sayantan/anaconda3/envs/ml/lib/python3.10/site-packages (from keras->keras-tuner) (0.0.9)\n",
      "Requirement already satisfied: h5py in /home/sayantan/anaconda3/envs/ml/lib/python3.10/site-packages (from keras->keras-tuner) (3.12.1)\n",
      "Requirement already satisfied: optree in /home/sayantan/anaconda3/envs/ml/lib/python3.10/site-packages (from keras->keras-tuner) (0.15.0)\n",
      "Requirement already satisfied: ml-dtypes in /home/sayantan/anaconda3/envs/ml/lib/python3.10/site-packages (from keras->keras-tuner) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sayantan/anaconda3/envs/ml/lib/python3.10/site-packages (from requests->keras-tuner) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sayantan/anaconda3/envs/ml/lib/python3.10/site-packages (from requests->keras-tuner) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sayantan/anaconda3/envs/ml/lib/python3.10/site-packages (from requests->keras-tuner) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sayantan/anaconda3/envs/ml/lib/python3.10/site-packages (from requests->keras-tuner) (2025.4.26)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/sayantan/anaconda3/envs/ml/lib/python3.10/site-packages (from optree->keras->keras-tuner) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/sayantan/anaconda3/envs/ml/lib/python3.10/site-packages (from rich->keras->keras-tuner) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/sayantan/anaconda3/envs/ml/lib/python3.10/site-packages (from rich->keras->keras-tuner) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/sayantan/anaconda3/envs/ml/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
      "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
      "Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdf0c4bb-819c-4b11-b3bc-f59b6a21b6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 03m 36s]\n",
      "val_accuracy: 0.7977863550186157\n",
      "\n",
      "Best val_accuracy So Far: 0.8779886960983276\n",
      "Total elapsed time: 00h 32m 52s\n",
      "\n",
      "==================================================\n",
      "BEST HYPERPARAMETERS FOUND:\n",
      "==================================================\n",
      "conv1_filters: 16\n",
      "conv1_kernel: 5\n",
      "conv2_filters: 32\n",
      "conv2_kernel: 7\n",
      "add_conv3: False\n",
      "dense_units: 32\n",
      "dropout_rate: 0.6000000000000001\n",
      "add_dense2: False\n",
      "learning_rate: 0.007752323623504601\n",
      "==================================================\n",
      "\n",
      "Training final model with best hyperparameters...\n",
      "Epoch 1/25\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 14ms/step - accuracy: 0.9055 - loss: 0.2915 - val_accuracy: 0.8052 - val_loss: 0.5107 - learning_rate: 0.0078\n",
      "Epoch 2/25\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 13ms/step - accuracy: 0.9609 - loss: 0.1235 - val_accuracy: 0.7834 - val_loss: 0.6172 - learning_rate: 0.0078\n",
      "Epoch 3/25\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 13ms/step - accuracy: 0.9679 - loss: 0.1021 - val_accuracy: 0.8068 - val_loss: 0.6494 - learning_rate: 0.0078\n",
      "Epoch 4/25\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 13ms/step - accuracy: 0.9680 - loss: 0.1014 - val_accuracy: 0.8453 - val_loss: 0.3986 - learning_rate: 0.0078\n",
      "Epoch 5/25\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 13ms/step - accuracy: 0.9706 - loss: 0.0902 - val_accuracy: 0.7939 - val_loss: 0.4069 - learning_rate: 0.0078\n",
      "Epoch 6/25\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 13ms/step - accuracy: 0.9733 - loss: 0.0879 - val_accuracy: 0.8404 - val_loss: 0.5981 - learning_rate: 0.0078\n",
      "Epoch 7/25\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 13ms/step - accuracy: 0.9730 - loss: 0.0865 - val_accuracy: 0.7785 - val_loss: 0.9752 - learning_rate: 0.0078\n",
      "Epoch 8/25\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 13ms/step - accuracy: 0.9771 - loss: 0.0747 - val_accuracy: 0.7724 - val_loss: 0.6260 - learning_rate: 0.0039\n",
      "Epoch 9/25\n",
      "\u001b[1m546/546\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - accuracy: 0.9785 - loss: 0.0682 - val_accuracy: 0.7693 - val_loss: 0.6648 - learning_rate: 0.0039\n",
      "\n",
      "Evaluating final model...\n",
      "\n",
      "FINAL RESULTS:\n",
      "Test Accuracy: 0.8199\n",
      "Test Loss: 0.9628\n",
      "Best Validation Accuracy: 0.8453\n",
      "\u001b[1m615/615\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.95      0.87     12312\n",
      "           1       0.87      0.61      0.72      7345\n",
      "\n",
      "    accuracy                           0.82     19657\n",
      "   macro avg       0.84      0.78      0.79     19657\n",
      "weighted avg       0.83      0.82      0.81     19657\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[11662   650]\n",
      " [ 2890  4455]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "HYPERPARAMETER TUNING COMPLETED!\n",
      "Best model saved with test accuracy: 0.8199\n",
      "Check MLflow UI for detailed experiment tracking\n",
      "============================================================\n",
      "🏃 View run ECG_Hyperparameter_Tuning_Experiment at: http://127.0.0.1:5002/#/experiments/0/runs/96744611c57a472ab621c49edc814ccd\n",
      "🧪 View experiment at: http://127.0.0.1:5002/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import keras_tuner as kt\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "data_path = \"mitbih\"  # Replace with your actual path\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5002/\")\n",
    "\n",
    "# --- Record List ---\n",
    "record_ids = ['100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '111', '112', '113', '114', '115',\n",
    "              '116', '117', '118', '119', '121', '122', '123', '124', '200', '201', '202', '203', '205', '207', '208',\n",
    "              '209', '210', '212', '213', '214', '215', '217', '219', '220', '221', '222', '223', '228', '230', '231',\n",
    "              '232', '233', '234']\n",
    "\n",
    "# --- Data Loading ---\n",
    "print(\"Loading ECG data...\")\n",
    "X = []\n",
    "y = []\n",
    "sample_record_ids = []\n",
    "\n",
    "for rec_id in record_ids:\n",
    "    try:\n",
    "        record = wfdb.rdrecord(os.path.join(data_path, rec_id))\n",
    "        annotation = wfdb.rdann(os.path.join(data_path, rec_id), 'atr')\n",
    "\n",
    "        signal = record.p_signal[:, 0]  # Lead I\n",
    "        ann_samples = annotation.sample\n",
    "        ann_symbols = annotation.symbol\n",
    "\n",
    "        for i, sample in enumerate(ann_samples):\n",
    "            if sample - 90 < 0 or sample + 90 > len(signal):\n",
    "                continue\n",
    "            beat_segment = signal[sample - 90: sample + 90]\n",
    "            label = 0 if ann_symbols[i] == 'N' else 1\n",
    "\n",
    "            X.append(beat_segment)\n",
    "            y.append(label)\n",
    "            sample_record_ids.append(rec_id)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping record {rec_id}: {e}\")\n",
    "\n",
    "X = np.array(X).reshape(-1, 180, 1)\n",
    "y = np.array(y)\n",
    "sample_record_ids = np.array(sample_record_ids)\n",
    "\n",
    "print(f\"Total samples loaded: {len(X)}\")\n",
    "print(f\"Class distribution - Normal: {np.sum(y == 0)}, Abnormal: {np.sum(y == 1)}\")\n",
    "\n",
    "# --- Convert record ID to index (0 to 47) ---\n",
    "record_id_to_index = {rec: idx for idx, rec in enumerate(record_ids)}\n",
    "sample_record_indices = np.array([record_id_to_index[rid] for rid in sample_record_ids])\n",
    "\n",
    "# --- Train/Val/Test Split (60/20/20) ---\n",
    "np.random.seed(42)\n",
    "all_indices = np.arange(len(record_ids))\n",
    "np.random.shuffle(all_indices)\n",
    "train_ids = all_indices[:29]\n",
    "val_ids = all_indices[29:39]\n",
    "test_ids = all_indices[39:]\n",
    "\n",
    "X_train = X[np.isin(sample_record_indices, train_ids)]\n",
    "y_train = y[np.isin(sample_record_indices, train_ids)]\n",
    "\n",
    "X_val = X[np.isin(sample_record_indices, val_ids)]\n",
    "y_val = y[np.isin(sample_record_indices, val_ids)]\n",
    "\n",
    "X_test = X[np.isin(sample_record_indices, test_ids)]\n",
    "y_test = y[np.isin(sample_record_indices, test_ids)]\n",
    "\n",
    "print(f\"Train samples: {len(X_train)}, Val samples: {len(X_val)}, Test samples: {len(X_test)}\")\n",
    "\n",
    "# --- Hyperparameter Tuning Model Builder ---\n",
    "def build_model(hp):\n",
    "    \"\"\"Build model with hyperparameters to tune\"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First Conv1D layer\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Choice('conv1_filters', values=[16, 32, 64]),\n",
    "        kernel_size=hp.Choice('conv1_kernel', values=[3, 5, 7]),\n",
    "        activation='relu',\n",
    "        input_shape=(180, 1)\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))\n",
    "    \n",
    "    # Second Conv1D layer\n",
    "    model.add(Conv1D(\n",
    "        filters=hp.Choice('conv2_filters', values=[32, 64, 128]),\n",
    "        kernel_size=hp.Choice('conv2_kernel', values=[3, 5, 7]),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(2))\n",
    "    \n",
    "    # Optional third Conv1D layer\n",
    "    if hp.Boolean('add_conv3'):\n",
    "        model.add(Conv1D(\n",
    "            filters=hp.Choice('conv3_filters', values=[64, 128, 256]),\n",
    "            kernel_size=hp.Choice('conv3_kernel', values=[3, 5]),\n",
    "            activation='relu'\n",
    "        ))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Dense(\n",
    "        units=hp.Choice('dense_units', values=[32, 64, 128, 256]),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(Dropout(hp.Float('dropout_rate', min_value=0.2, max_value=0.7, step=0.1)))\n",
    "    \n",
    "    # Optional second dense layer\n",
    "    if hp.Boolean('add_dense2'):\n",
    "        model.add(Dense(\n",
    "            units=hp.Choice('dense2_units', values=[16, 32, 64]),\n",
    "            activation='relu'\n",
    "        ))\n",
    "        model.add(Dropout(hp.Float('dropout2_rate', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# --- Custom Tuner with MLflow Integration ---\n",
    "class MLflowTuner(kt.RandomSearch):\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "        # Start MLflow run for this trial\n",
    "        with mlflow.start_run(run_name=f\"trial_{trial.trial_id}\", nested=True):\n",
    "            # Log hyperparameters\n",
    "            hp_values = trial.hyperparameters.values\n",
    "            for param, value in hp_values.items():\n",
    "                mlflow.log_param(param, value)\n",
    "            \n",
    "            # Run the trial\n",
    "            result = super().run_trial(trial, *args, **kwargs)\n",
    "            \n",
    "            # Log the best validation accuracy for this trial\n",
    "            if hasattr(result, 'history') and 'val_accuracy' in result.history:\n",
    "                best_val_acc = max(result.history['val_accuracy'])\n",
    "                mlflow.log_metric(\"best_val_accuracy\", best_val_acc)\n",
    "                mlflow.log_metric(\"final_val_accuracy\", result.history['val_accuracy'][-1])\n",
    "            \n",
    "            return result\n",
    "\n",
    "# --- Hyperparameter Tuning ---\n",
    "print(\"Starting hyperparameter tuning...\")\n",
    "\n",
    "# Create tuner\n",
    "tuner = MLflowTuner(\n",
    "    build_model,\n",
    "    objective=kt.Objective('val_accuracy', direction='max'),\n",
    "    max_trials=20,  # Adjust based on your computational resources\n",
    "    directory='ecg_tuning',\n",
    "    project_name='ecg_cnn_hyperparameter_search',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Start parent MLflow run\n",
    "with mlflow.start_run(run_name=\"ECG_Hyperparameter_Tuning_Experiment\"):\n",
    "    mlflow.log_param(\"total_trials\", 20)\n",
    "    mlflow.log_param(\"tuning_objective\", \"val_accuracy\")\n",
    "    mlflow.log_param(\"train_records\", len(train_ids))\n",
    "    mlflow.log_param(\"val_records\", len(val_ids))\n",
    "    mlflow.log_param(\"test_records\", len(test_ids))\n",
    "    \n",
    "    # Search for best hyperparameters\n",
    "    tuner.search(\n",
    "        X_train, y_train,\n",
    "        epochs=15,  # Reduced epochs for faster tuning\n",
    "        batch_size=128,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=1,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n",
    "    )\n",
    "    \n",
    "    # Get best hyperparameters\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"BEST HYPERPARAMETERS FOUND:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    best_hp_dict = {}\n",
    "    for param in ['conv1_filters', 'conv1_kernel', 'conv2_filters', 'conv2_kernel', \n",
    "                  'add_conv3', 'dense_units', 'dropout_rate', 'add_dense2', 'learning_rate']:\n",
    "        if param in best_hps.values:\n",
    "            value = best_hps.get(param)\n",
    "            best_hp_dict[param] = value\n",
    "            print(f\"{param}: {value}\")\n",
    "            mlflow.log_param(f\"best_{param}\", value)\n",
    "    \n",
    "    # Additional conv3 parameters if enabled\n",
    "    if best_hps.get('add_conv3'):\n",
    "        for param in ['conv3_filters', 'conv3_kernel']:\n",
    "            if param in best_hps.values:\n",
    "                value = best_hps.get(param)\n",
    "                best_hp_dict[param] = value\n",
    "                print(f\"{param}: {value}\")\n",
    "                mlflow.log_param(f\"best_{param}\", value)\n",
    "    \n",
    "    # Additional dense2 parameters if enabled\n",
    "    if best_hps.get('add_dense2'):\n",
    "        for param in ['dense2_units', 'dropout2_rate']:\n",
    "            if param in best_hps.values:\n",
    "                value = best_hps.get(param)\n",
    "                best_hp_dict[param] = value\n",
    "                print(f\"{param}: {value}\")\n",
    "                mlflow.log_param(f\"best_{param}\", value)\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # --- Train Final Model with Best Hyperparameters ---\n",
    "    print(\"\\nTraining final model with best hyperparameters...\")\n",
    "    \n",
    "    # Build and train the best model with more epochs\n",
    "    best_model = tuner.hypermodel.build(best_hps)\n",
    "    \n",
    "    # Train with best hyperparameters\n",
    "    history = best_model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=25,  # More epochs for final training\n",
    "        batch_size=128,\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.5, min_lr=1e-7)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # --- Final Evaluation ---\n",
    "    print(\"\\nEvaluating final model...\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    # Log final metrics\n",
    "    mlflow.log_metric(\"final_test_loss\", test_loss)\n",
    "    mlflow.log_metric(\"final_test_accuracy\", test_accuracy)\n",
    "    mlflow.log_metric(\"final_train_accuracy\", max(history.history['accuracy']))\n",
    "    mlflow.log_metric(\"final_val_accuracy\", max(history.history['val_accuracy']))\n",
    "    \n",
    "    print(f\"\\nFINAL RESULTS:\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Best Validation Accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "    \n",
    "    # Predictions and detailed evaluation\n",
    "    y_pred_proba = best_model.predict(X_test)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    print(f\"\\nClassification Report:\\n{report}\")\n",
    "    \n",
    "    with open(\"best_model_classification_report.txt\", \"w\") as f:\n",
    "        f.write(\"BEST HYPERPARAMETERS:\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        for param, value in best_hp_dict.items():\n",
    "            f.write(f\"{param}: {value}\\n\")\n",
    "        f.write(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        f.write(\"CLASSIFICATION REPORT:\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        f.write(report)\n",
    "    mlflow.log_artifact(\"best_model_classification_report.txt\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"\\nConfusion Matrix:\\n{conf_mat}\")\n",
    "    \n",
    "    with open(\"best_model_confusion_matrix.txt\", \"w\") as f:\n",
    "        f.write(\"CONFUSION MATRIX:\\n\")\n",
    "        f.write(np.array2string(conf_mat))\n",
    "    mlflow.log_artifact(\"best_model_confusion_matrix.txt\")\n",
    "    \n",
    "    # Plot Training History\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "    plt.title('Model Accuracy (Best Hyperparameters)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss', marker='o')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "    plt.title('Model Loss (Best Hyperparameters)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Learning rate plot (if available)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    if 'lr' in history.history:\n",
    "        plt.plot(history.history['lr'], label='Learning Rate', marker='d')\n",
    "        plt.title('Learning Rate Schedule')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.yscale('log')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'Learning Rate\\nNot Tracked', \n",
    "                ha='center', va='center', transform=plt.gca().transAxes)\n",
    "        plt.title('Learning Rate')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"best_model_training_history.png\", dpi=300, bbox_inches='tight')\n",
    "    mlflow.log_artifact(\"best_model_training_history.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Save the best model\n",
    "    best_model.save(\"best_ecg_model.h5\")\n",
    "    mlflow.log_artifact(\"best_ecg_model.h5\")\n",
    "    \n",
    "    # Summary of tuning results\n",
    "    tuning_summary = f\"\"\"\n",
    "HYPERPARAMETER TUNING SUMMARY\n",
    "==============================\n",
    "Total Trials: 20\n",
    "Best Validation Accuracy: {max(history.history['val_accuracy']):.4f}\n",
    "Final Test Accuracy: {test_accuracy:.4f}\n",
    "\n",
    "Best Hyperparameters:\n",
    "{'-'*30}\n",
    "\"\"\"\n",
    "    for param, value in best_hp_dict.items():\n",
    "        tuning_summary += f\"{param}: {value}\\n\"\n",
    "    \n",
    "    with open(\"hyperparameter_tuning_summary.txt\", \"w\") as f:\n",
    "        f.write(tuning_summary)\n",
    "    mlflow.log_artifact(\"hyperparameter_tuning_summary.txt\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"HYPERPARAMETER TUNING COMPLETED!\")\n",
    "    print(f\"Best model saved with test accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"Check MLflow UI for detailed experiment tracking\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81fc621-4e9c-4104-b096-a1d2f687eb71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
